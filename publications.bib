
@article{merzon_temporal_2020,
	title = {Temporal {Limitations} of the {Standard} {Leaky} {Integrate} and {Fire} {Model}},
	volume = {10},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/2076-3425/10/1/16},
	doi = {10.3390/brainsci10010016},
	abstract = {Itti and Koch\&rsquo;s Saliency Model has been used extensively to simulate fixation selection in a variety of tasks from visual search to simple reaction times. Although the Saliency Model has been tested for its spatial prediction of fixations in visual salience, it has not been well tested for their temporal accuracy. Visual tasks, like search, invariably result in a positively skewed distribution of saccadic reaction times over large numbers of samples, yet we show that the leaky integrate and fire (LIF) neuronal model included in the classic implementation of the model tends to produce a distribution shifted to shorter fixations (in comparison with human data). Further, while parameter optimization using a genetic algorithm and Nelder\&ndash;Mead method does improve the fit of the resulting distribution, it is still unable to match temporal distributions of human responses in a visual task. Analysis of times for individual images reveal that the LIF algorithm produces initial fixation durations that are fixed instead of a sample from a distribution (as in the human case). Only by aggregating responses over many input images do they result in a distribution, although the form of this distribution still depends on the input images used to create it and not on internal model variability.},
	language = {en},
	number = {1},
	urldate = {2022-01-18},
	journal = {Brain Sciences},
	author = {Merzon, Liya and Malevich, Tatiana and Zhulikov, Georgiy and Krasovskaya, Sofia and MacInnes, W. Joseph},
	month = jan,
	year = {2020},
	note = {Number: 1
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {leaky integrate and fire model, saccade generation, salience model, visual search},
	pages = {16},
	file = {Full Text PDF:C\:\\Users\\sofia\\Zotero\\storage\\AMLTGE78\\Merzon et al. - 2020 - Temporal Limitations of the Standard Leaky Integra.pdf:application/pdf},
}

@article{krasovskaya_salience_2019,
	title = {Salience {Models}: {A} {Computational} {Cognitive} {Neuroscience} {Review}},
	volume = {3},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	shorttitle = {Salience {Models}},
	url = {https://www.mdpi.com/2411-5150/3/4/56},
	doi = {10.3390/vision3040056},
	abstract = {The seminal model by Laurent Itti and Cristoph Koch demonstrated that we can compute the entire flow of visual processing from input to resulting fixations. Despite many replications and follow-ups, few have matched the impact of the original model\&mdash;so what made this model so groundbreaking? We have selected five key contributions that distinguish the original salience model by Itti and Koch; namely, its contribution to our theoretical, neural, and computational understanding of visual processing, as well as the spatial and temporal predictions for fixation distributions. During the last 20 years, advances in the field have brought up various techniques and approaches to salience modelling, many of which tried to improve or add to the initial Itti and Koch model. One of the most recent trends has been to adopt the computational power of deep learning neural networks; however, this has also shifted their primary focus to spatial classification. We present a review of recent approaches to modelling salience, starting from direct variations of the Itti and Koch salience model to sophisticated deep-learning architectures, and discuss the models from the point of view of their contribution to computational cognitive neuroscience.},
	language = {en},
	number = {4},
	urldate = {2022-01-18},
	journal = {Vision},
	author = {Krasovskaya, Sofia and MacInnes, W. Joseph},
	month = dec,
	year = {2019},
	note = {Number: 4
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {computational modelling, deep learning, Itti and Koch, salience},
	pages = {56},
	file = {Full Text PDF:C\:\\Users\\sofia\\Zotero\\storage\\DKAB9XFA\\Krasovskaya and MacInnes - 2019 - Salience Models A Computational Cognitive Neurosc.pdf:application/pdf;Snapshot:C\:\\Users\\sofia\\Zotero\\storage\\V5XH5PAZ\\56.html:text/html},
}

@misc{krasovskaya_deep_2021,
	title = {Deep {Learning} {Neural} {Networks} as a {Component} of a {Model} of {Saccadic} {Generation}},
	copyright = {All rights reserved},
	url = {https://psyarxiv.com/zr6ma/},
	doi = {10.31234/osf.io/zr6ma},
	abstract = {Approximately twenty years ago, Laurent Itti and Christof Koch created a model of saliency in visual attention in an attempt to recreate the work of biological pyramidal neurons by mimicking neurons with centre-surround receptive fields. The Saliency Model has launched many studies that contributed to the understanding of layers of vision and the sphere of visual attention. The aim of the current study is to improve this model by using an artificial neural network as the spatial component of a model that generates saccades similar to how humans make saccadic eye movements. The proposed model uses a Leaky Integrate-and-Fire layer for temporal predictions, and replaces parallel feature maps with a deep learning neural network in order to create a generative model that is precise for both spatial and temporal shifts of attention. Our model was able to predict eye movements based on unsupervised learning from raw image input, combined with supervised learning from fixation maps retrieved during an eye-tracking experiment. The results imply that it is possible to match the spatial and temporal distributions of the model to spatial and temporal human distributions.},
	language = {en-us},
	urldate = {2022-01-18},
	publisher = {PsyArXiv},
	author = {Krasovskaya, Sofia and Zhulikov, Georgii and MacInnes, Joseph},
	month = mar,
	year = {2021},
	doi = {10.31234/osf.io/zr6ma},
	note = {type: article},
	keywords = {Cognitive Neuroscience, Computational Neuroscience, deep learning neural networks, Itti and Koch, leaky integrate-and-fire, Life Sciences, Neuroscience, saccadic generation, saliency models, visual attention},
	file = {Full Text PDF:C\:\\Users\\sofia\\Zotero\\storage\\LHHHD29L\\Krasovskaya et al. - 2021 - Deep Learning Neural Networks as a Component of a .pdf:application/pdf},
}

@article{krasovskaya_microsaccade_2023,
	title = {Microsaccade rate activity during the preparation of pro- and antisaccades},
	copyright = {All rights reserved},
	issn = {1943-393X},
	url = {https://doi.org/10.3758/s13414-023-02731-3},
	doi = {10.3758/s13414-023-02731-3},
	abstract = {Microsaccades belong to the category of fixational micromovements and may be crucial for image stability on the retina. Eye movement paradigms typically require fixational control, but this does not eliminate all oculomotor activity. The antisaccade task requires a planned eye movement in the direction opposite of an onset, allowing separation of planning and execution. We build on previous studies of microsaccades in the antisaccade task using a combination of fixed and mixed pro- and antisaccade blocks. We hypothesized that microsaccade rates may be reduced prior to the execution of antisaccades as compared with regular saccades (prosaccades). In two experiments, we measured microsaccades in four conditions across three trial blocks: one block each of fixed prosaccade and antisaccade trials, and a mixed block where both saccade types were randomized. We anticipated that microsaccade rates would be higher prior to antisaccades than prosaccades due to the need to preemptively suppress reflexive saccades during antisaccade generation. In Experiment 1, with monocular eye tracking, there was an interaction between the effects of saccade and block type on microsaccade rates, suggesting lower rates on antisaccade trials, but only within mixed blocks. In Experiment 2, eye tracking was binocular, revealing suppressed microsaccade rates on antisaccade trials. A cluster permutation analysis of the microsaccade rate over the course of a trial did not reveal any particular critical time for this difference in microsaccade rates. Our findings suggest that microsaccade rates reflect the degree of suppression of the oculomotor system during the antisaccade task.},
	language = {en},
	urldate = {2023-07-02},
	journal = {Attention, Perception, \& Psychophysics},
	author = {Krasovskaya, Sofia and Kristjánsson, Árni and MacInnes, W. Joseph},
	month = may,
	year = {2023},
	keywords = {Antisaccade task, Executive control, Microsaccades, Oculomotor suppression},
	file = {Full Text PDF:C\:\\Users\\sofia\\Zotero\\storage\\MCQX5CS7\\Krasovskaya et al. - 2023 - Microsaccade rate activity during the preparation .pdf:application/pdf},
}

@article{krasovskaya_assessing_2024,
	title = {Assessing the {Size} of the {Functional} {Field} of {View} in a {Gaze}-{Contingent} {Search} {Paradigm}},
	volume = {8},
	copyright = {All rights reserved},
	doi = {https://doi.org/10.1145/3655597},
	journal = {ACM Hum.-Comput. Interact.},
	author = {Krasovskaya, Sofia and Kristjánsson, Árni and MacInnes, W. Joseph},
	month = may,
	year = {2024},
	pages = {17},
}
